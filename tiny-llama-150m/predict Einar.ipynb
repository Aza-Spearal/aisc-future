{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costest(l1,l2):\n",
    "    cos=nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    c=0.0\n",
    "    a=0.0\n",
    "    for j in range(len(l1[:,0])):\n",
    "        a+=cos(l1[j,:],l2[j,:])\n",
    "        c+=1.0\n",
    "    return a/c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(ImprovedTwoLayerNN, self).__init__()\n",
    "        # Increase depth and capacity\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Adjust dropout rate as needed\n",
    "        \n",
    "        # Additional layer\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Adjust dropout rate as needed\n",
    "        \n",
    "        # Output layer\n",
    "        self.layer3 = nn.Linear(hidden_size * 2, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.layer2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        return x\n",
    "    \n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,  output_size):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data_in, data_out, model, criterion, num_epochs, save=False, learning_rate=0.001):\n",
    "    \n",
    "    \n",
    "    # Create a complete dataset\n",
    "    full_dataset = TensorDataset(data_in.to(\"cuda\"), data_out.to(\"cuda\"))\n",
    "\n",
    "    # Define the sizes for your training and validation sets\n",
    "    total_size = len(full_dataset)\n",
    "    train_size = int(0.8 * total_size)\n",
    "    val_size = total_size - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    # Create DataLoaders for both training and validation sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=400, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=500)  # No need to shuffle the validation data\n",
    "\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "     \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs, targets = inputs.to(\"cuda\").float(), targets.to(\"cuda\").float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            #loss.backward()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            \n",
    "         \n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "        epoch_loss = running_loss / len(train_dataloader.dataset)\n",
    "        # Validation phase\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        with torch.no_grad():  # No gradients need to be calculated\n",
    "            for inputs, targets in val_dataloader:\n",
    "                inputs, targets = inputs.to(\"cuda\").float(), targets.to(\"cuda\").float()\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                loss = criterion(outputs, targets)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "        val_loss = running_loss / len(val_dataloader.dataset)\n",
    "    print(costest(outputs,targets))\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.8f}, Validation Loss: {val_loss:.8f}')\n",
    "    \n",
    "    if(save):\n",
    "        # Ensure the model is in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        predictions = []\n",
    "        with torch.no_grad():  # No gradients needed for inference\n",
    "            for inputs, _ in full_dataset:  # Assuming your dataset returns inputs and targets\n",
    "                inputs = inputs.to('cuda').float().unsqueeze(0)\n",
    "                \n",
    "                # Get the model output\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "\n",
    "                predictions.append(outputs.cpu())\n",
    "\n",
    "        # Concatenate all batches of predictions\n",
    "        all_predictions = torch.stack(predictions, dim=0)\n",
    "\n",
    "        # Save the tensor to a file\n",
    "        torch.save(all_predictions, 'model_predictions.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"1\"\n",
    "\n",
    "loaded_activations  = torch.load(f'activations-{id}.pth')\n",
    "loaded_embeddings = torch.load(f\"embeddings-{id}.pth\")\n",
    "loaded_residual_stream = torch.load(f\"residual_stream-{id}.pth\")\n",
    "#loaded_rebased_embeddings = torch.load(f\"rebased_embeddings-{id}.pth\")\n",
    "loaded_one_hot = torch.load(f\"one-hot-{id}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_in' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_in\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_in' is not defined"
     ]
    }
   ],
   "source": [
    "data_in.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0051, grad_fn=<DivBackward0>)\n",
      "tensor(0.0831, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "loaded_embeddings=loaded_embeddings.to(dtype=torch.float32)\n",
    "\n",
    "bn = torch.nn.BatchNorm1d(4096)\n",
    "loaded_embeddings = bn(loaded_embeddings)\n",
    "def cossimtest(l):\n",
    "    cos=nn.CosineSimilarity(dim=0, eps=1e-6)\n",
    "    c=0.0\n",
    "    a=0.0\n",
    "    for j in range(len(l[:,0])):\n",
    "        for i in range(len(l[:,0])):\n",
    "            if(i<j):\n",
    "                a+=cos(l[i,:],l[j,:])\n",
    "                c+=1.0\n",
    "    print(a/c)\n",
    "\n",
    "cossimtest(loaded_embeddings)\n",
    "cossimtest(loaded_one_hot.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([195, 4096])\n",
      "torch.Size([21, 195])\n"
     ]
    }
   ],
   "source": [
    "print(loaded_embeddings.size())\n",
    "print(loaded_one_hot.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n",
      "0\n",
      "tensor(-0.1431, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.99456662, Validation Loss: 1.02705252\n",
      "1\n",
      "tensor(0.1003, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.47711614, Validation Loss: 1.17776179\n",
      "2\n",
      "tensor(0.0796, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.41598642, Validation Loss: 1.24047458\n",
      "3\n",
      "tensor(0.0969, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.04650988, Validation Loss: 1.37019670\n",
      "4\n",
      "tensor(0.0755, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.06309481, Validation Loss: 1.33092928\n",
      "5\n",
      "tensor(0.0742, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.05800305, Validation Loss: 1.35214448\n",
      "6\n",
      "tensor(0.0909, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.12691428, Validation Loss: 1.25419056\n",
      "7\n",
      "tensor(0.1261, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.09117375, Validation Loss: 1.30883729\n",
      "8\n",
      "tensor(0.1251, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.16916449, Validation Loss: 1.24366927\n",
      "9\n",
      "tensor(0.1650, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.07839979, Validation Loss: 1.22253859\n",
      "10\n",
      "tensor(0.1385, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.10616701, Validation Loss: 1.24920309\n",
      "11\n",
      "tensor(0.1752, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.03687004, Validation Loss: 1.22781241\n",
      "12\n",
      "tensor(0.1286, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.04222106, Validation Loss: 1.26918662\n",
      "13\n",
      "tensor(0.1532, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.02480413, Validation Loss: 1.21913576\n",
      "14\n",
      "tensor(0.1374, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.02940838, Validation Loss: 1.24076319\n",
      "15\n",
      "tensor(0.1566, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.02329141, Validation Loss: 1.26132715\n",
      "16\n",
      "tensor(0.1300, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.04909126, Validation Loss: 1.31933212\n",
      "17\n",
      "tensor(0.1495, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.03643695, Validation Loss: 1.29455388\n",
      "18\n",
      "tensor(0.1440, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.05355931, Validation Loss: 1.26301920\n",
      "19\n",
      "tensor(0.1219, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.04023090, Validation Loss: 1.27668285\n",
      "20\n",
      "tensor(0.1648, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.08150106, Validation Loss: 1.23836029\n",
      "21\n",
      "tensor(0.1627, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.08341274, Validation Loss: 1.22102511\n",
      "22\n",
      "tensor(0.1302, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.14477402, Validation Loss: 1.27560544\n",
      "23\n",
      "tensor(0.1646, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 0.26592717, Validation Loss: 1.15305614\n",
      "24\n",
      "tensor(-0.0636, device='cuda:0')\n",
      "Epoch [100/100], Training Loss: 1.00328922, Validation Loss: 0.98585314\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 25 is out of bounds for dimension 0 with size 25",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m\n\u001b[1;32m      5\u001b[0m ff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     residual_stream \u001b[38;5;241m=\u001b[39m  [i[layer, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loaded_residual_stream]\n\u001b[1;32m     15\u001b[0m     residual_stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(residual_stream)\n\u001b[1;32m     19\u001b[0m     data_in \u001b[38;5;241m=\u001b[39m residual_stream\n",
      "Cell \u001b[0;32mIn[30], line 11\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m ff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     residual_stream \u001b[38;5;241m=\u001b[39m  [\u001b[43mi\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loaded_residual_stream]\n\u001b[1;32m     15\u001b[0m     residual_stream \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(residual_stream)\n\u001b[1;32m     19\u001b[0m     data_in \u001b[38;5;241m=\u001b[39m residual_stream\n",
      "\u001b[0;31mIndexError\u001b[0m: index 25 is out of bounds for dimension 0 with size 25"
     ]
    }
   ],
   "source": [
    "print(len(loaded_residual_stream[0]))\n",
    "\n",
    "# consts\n",
    "attention = 2\n",
    "ff = 3\n",
    "\n",
    "\n",
    "for layer in range(30):\n",
    "\n",
    "\n",
    "    residual_stream =  [i[layer, -1, :] for i in loaded_residual_stream]\n",
    "\n",
    " \n",
    "\n",
    "    residual_stream = torch.stack(residual_stream)\n",
    "\n",
    "    \n",
    "\n",
    "    data_in = residual_stream\n",
    "    #data_in=loaded_activations\n",
    "    #data_out = loaded_one_hot.T\n",
    "    data_out = loaded_embeddings\n",
    "\n",
    "    input_size = data_in.shape[1]\n",
    "    output_size = data_out.shape[1]\n",
    "    hidden_size = 8000\n",
    "\n",
    "    criterion = nn.MSELoss() \n",
    "\n",
    "    print(layer)\n",
    "    model = TwoLayerNN(input_size, hidden_size, output_size)\n",
    "    train(data_in, data_out, model, criterion, num_epochs=100)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mistral2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
